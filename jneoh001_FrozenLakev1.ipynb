{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzrTbcS4aqLl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\")"
      ],
      "metadata": {
        "id": "BpRSxDX6a0DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "\n",
        "print(action_space_size)\n",
        "print(state_space_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFP6WhrzbFOe",
        "outputId": "b4db8433-a24a-4749-9dd6-344b3e235a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puVGhWcAa4TZ",
        "outputId": "9a8b3bee-733c-413e-88b7-a20ae144db10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#total number of episodes we want agent to play for training\n",
        "num_episodes = 10000\n",
        "#max steps agent can take before termination\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "\n",
        "learning_rate = 0.1 #alpha\n",
        "discount_rate = 0.92 #gamma\n",
        "\n",
        "# handles a balance between exploration and exploitation\n",
        "exploration_rate = 1\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001"
      ],
      "metadata": {
        "id": "v-qVQpqxbY6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#track scores \n",
        "rewards_all_episodes = []\n",
        "\n",
        "\n",
        "#Q-Learning Algorithm\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  state = env.reset()\n",
        "\n",
        "  done = False # tracks whether episode finished or not\n",
        "  rewards_current_episode = 0 \n",
        "\n",
        "  for step in range(max_steps_per_episode):\n",
        "    # Exploration-exploitation trade off\n",
        "    exploration_rate_threshold = random.uniform(0, 1)\n",
        "    if exploration_rate_threshold > exploration_rate:\n",
        "      action = np.argmax(q_table[state,:])   # exploit the environment and chooses action with highest q-value at this state\n",
        "    else:\n",
        "      action = env.action_space.sample() # agent will explore the environment. sample an action randomly.\n",
        "    \n",
        "    new_state, reward, done, info = env.step(action)    \n",
        "\n",
        "    #Update Q-Table for Q(s,a)\n",
        "    q_table[state, action] = q_table[state,action] * (1 - learning_rate) + \\\n",
        "       learning_rate * ( reward + discount_rate * np.max(q_table[new_state, :]))\n",
        "    \n",
        "    state = new_state\n",
        "    rewards_current_episode += reward\n",
        "\n",
        "    if done == True:\n",
        "      break\n",
        "    \n",
        "  #Exploration Rate Decay\n",
        "  exploration_rate = min_exploration_rate + \\\n",
        "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
        "  \n",
        "  rewards_all_episodes.append(rewards_current_episode)"
      ],
      "metadata": {
        "id": "ysMGUCocc133"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print the average reward per thousand episodes\n",
        "\n",
        "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
        "count = 1000\n",
        "print(\"Average Rewards per thousand episodes \\n\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "  print(count, \":\", str(sum(r/1000)))\n",
        "  count += 1000\n",
        "\n",
        "#Print updated q-table\n",
        "\n",
        "print(\"\\n Q-Table \\n\")\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXcHhDBCeqoj",
        "outputId": "6aa961f4-6a1f-4fdd-d073-b9079fc6d497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Rewards per thousand episodes \n",
            "\n",
            "1000 : 0.03100000000000002\n",
            "2000 : 0.16800000000000012\n",
            "3000 : 0.3110000000000002\n",
            "4000 : 0.3940000000000003\n",
            "5000 : 0.4240000000000003\n",
            "6000 : 0.5520000000000004\n",
            "7000 : 0.4850000000000004\n",
            "8000 : 0.5370000000000004\n",
            "9000 : 0.5440000000000004\n",
            "10000 : 0.5180000000000003\n",
            "\n",
            " Q-Table \n",
            "\n",
            "[[0.05247629 0.05531855 0.0590736  0.05262206]\n",
            " [0.04619486 0.04541249 0.04956274 0.05550541]\n",
            " [0.05987857 0.06533467 0.05937194 0.05983874]\n",
            " [0.04225558 0.03678994 0.03375975 0.05289591]\n",
            " [0.07592051 0.06264567 0.05931405 0.05777355]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.1032314  0.06691335 0.05197833 0.02135513]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.08479011 0.08666076 0.09641203 0.11992831]\n",
            " [0.16859413 0.19581029 0.17326242 0.16737243]\n",
            " [0.24491709 0.1945606  0.17958997 0.11528884]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.22258662 0.24362088 0.32690583 0.26202922]\n",
            " [0.44012138 0.43255135 0.4293258  0.5190715 ]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for episode in range(3):\n",
        "#   state = env.reset()\n",
        "#   done = False\n",
        "#   print(\"* EPISODE\", episode+1, \"*******\\n\")\n",
        "#   time.sleep(1)\n",
        "\n",
        "#   for step in range(max_steps_per_episode):\n",
        "#     clear_output(wait=True)\n",
        "#     env.render()\n",
        "#     time.sleep(0.3)\n",
        "\n",
        "#     action = np.argmax(q_table[state,:])\n",
        "#     new_state, reward, done, info = env.step(action)\n",
        "\n",
        "#     if done:\n",
        "#       clear_output(wait=True)\n",
        "#       env.render()\n",
        "#       if reward == 1:\n",
        "#         print(\"Reached goal !\")\n",
        "#         time.sleep(3)\n",
        "#       else:\n",
        "#         print(\"Fell through a hole\")\n",
        "#         time.sleep(3)\n",
        "#       clear_output(wait=True)\n",
        "#       break\n",
        "\n",
        "#       state = new_satate\n",
        "# env.close()\n"
      ],
      "metadata": {
        "id": "InWcCEILesxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQAWXzSrttkN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}